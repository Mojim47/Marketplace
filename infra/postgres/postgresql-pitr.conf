# ═══════════════════════════════════════════════════════════════════════════
# NextGen Marketplace - PostgreSQL Point-in-Time Recovery Configuration
# ═══════════════════════════════════════════════════════════════════════════
# Purpose: Enable WAL archiving for Point-in-Time Recovery (PITR)
# Requirements: 9.4 - Support point-in-time recovery for the last 30 days
# ═══════════════════════════════════════════════════════════════════════════

# ───────────────────────────────────────────────────────────────────────────
# WAL Settings
# ───────────────────────────────────────────────────────────────────────────

# Set WAL level to replica for archiving (required for PITR)
wal_level = replica

# Enable archiving
archive_mode = on

# Archive command - copies WAL files to archive directory
# %p = path to WAL file, %f = filename
archive_command = 'test ! -f /var/lib/postgresql/wal_archive/%f && cp %p /var/lib/postgresql/wal_archive/%f'

# Archive timeout - force archive after this many seconds even if WAL not full
archive_timeout = 300

# ───────────────────────────────────────────────────────────────────────────
# WAL Size Settings
# ───────────────────────────────────────────────────────────────────────────

# Minimum WAL size to retain
min_wal_size = 1GB

# Maximum WAL size before checkpoint
max_wal_size = 4GB

# WAL segment size (default 16MB)
# wal_segment_size = 16MB  # Cannot be changed after initdb

# Number of WAL files to keep for standby servers
wal_keep_size = 1GB

# ───────────────────────────────────────────────────────────────────────────
# Checkpoint Settings
# ───────────────────────────────────────────────────────────────────────────

# Checkpoint completion target (0.0 - 1.0)
# Higher values spread checkpoint I/O over longer time
checkpoint_completion_target = 0.9

# Maximum time between automatic checkpoints
checkpoint_timeout = 10min

# Maximum WAL size before forcing checkpoint
# max_wal_size = 4GB  # Already set above

# ───────────────────────────────────────────────────────────────────────────
# Replication Settings (for streaming replication)
# ───────────────────────────────────────────────────────────────────────────

# Maximum number of concurrent connections from standby servers
max_wal_senders = 5

# Maximum number of replication slots
max_replication_slots = 5

# Enable hot standby for read queries on replica
hot_standby = on

# Feedback from standby servers
hot_standby_feedback = on

# ───────────────────────────────────────────────────────────────────────────
# Recovery Settings
# ───────────────────────────────────────────────────────────────────────────

# Restore command for recovery (used during PITR)
# restore_command = 'cp /var/lib/postgresql/wal_archive/%f %p'

# Recovery target settings (uncomment and modify for PITR)
# recovery_target_time = '2024-01-15 10:00:00 Asia/Tehran'
# recovery_target_timeline = 'latest'
# recovery_target_action = 'promote'

# ───────────────────────────────────────────────────────────────────────────
# Logging Settings for Recovery
# ───────────────────────────────────────────────────────────────────────────

# Log checkpoints
log_checkpoints = on

# Log replication commands
log_replication_commands = on

# ═══════════════════════════════════════════════════════════════════════════
# Usage Instructions:
# ═══════════════════════════════════════════════════════════════════════════
#
# 1. Include this file in postgresql.conf:
#    include = '/etc/postgresql/conf.d/postgresql-pitr.conf'
#
# 2. Create WAL archive directory:
#    mkdir -p /var/lib/postgresql/wal_archive
#    chown postgres:postgres /var/lib/postgresql/wal_archive
#    chmod 700 /var/lib/postgresql/wal_archive
#
# 3. Restart PostgreSQL:
#    pg_ctl restart -D $PGDATA
#
# 4. To perform Point-in-Time Recovery:
#    a. Stop PostgreSQL
#    b. Create recovery.signal file in data directory
#    c. Set restore_command and recovery_target_time in postgresql.conf
#    d. Start PostgreSQL
#
# 5. To clean old WAL archives (keep 30 days):
#    find /var/lib/postgresql/wal_archive -mtime +30 -delete
#
# ═══════════════════════════════════════════════════════════════════════════
