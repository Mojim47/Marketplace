# ğŸ“– Runbooks & Playbooks - Incident Response & Operations

## Table of Contents

1. [On-Call Procedures](#on-call-procedures)
2. [Incident Response Playbooks](#incident-response-playbooks)
3. [Deployment Guide](#deployment-guide)
4. [Troubleshooting Steps](#troubleshooting-steps)
5. [Escalation Paths](#escalation-paths)

---

## On-Call Procedures

### On-Call Rotation

```
Week 1: Team A (Primary: @alice, Secondary: @bob)
Week 2: Team B (Primary: @charlie, Secondary: @diana)
Week 3: Team C (Primary: @eve, Secondary: @frank)
Week 4: Team A (rotation repeats)

Handoff Timing: Monday 09:00 UTC
Escalation: Alert â†’ 5min â†’ Primary â†’ 10min â†’ Secondary â†’ 15min â†’ Manager
```

### First 5 Minutes (Triage)

```
â˜ Acknowledge alert (click link in Slack/PagerDuty)
â˜ Open dashboard: https://grafana.nextgen.local/d/prod-overview
â˜ Check error rate: Check if > baseline (usually <0.1%)
â˜ Check latency: Check if p99 > 1000ms
â˜ Check resources: CPU/Memory/Disk on nodes
â˜ Review recent deployments (last 30 minutes)
â˜ Check if incident is known/acknowledged
â˜ Post in #incidents channel: "Incident acknowledged by @oncall"
```

### 10-Minute Assessment

```
â˜ Determine severity:
  â”œâ”€ P1: Complete outage, all users affected, 10+ errors/sec
  â”œâ”€ P2: Partial outage, some users affected, errors < 10/sec
  â”œâ”€ P3: Degraded performance, p99 > 2000ms
  â””â”€ P4: Minor issues, no user impact
  
â˜ Check logs:
  â””â”€ kubectl logs -f deployment/nextgen-api -n production
  â””â”€ Look for: error stack trace, database connection, timeout
  
â˜ Identify affected service:
  â”œâ”€ API: Check 3000/health, database connectivity
  â”œâ”€ Database: Check CPU, connections, replication lag
  â”œâ”€ Cache: Check Redis PING, memory usage
  â”œâ”€ Ingress: Check request rate, blocked connections
  â””â”€ Other: Identify from metrics
  
â˜ Decide: Can fix in <5 minutes? â†’ Fix directly
  â˜ If yes: Proceed to "Quick Fix" section
  â˜ If no: Escalate to team lead within 15 minutes
```

### Communication Template (Slack)

```
ğŸš¨ Incident P{1-4}: {Service} {Issue}

Severity: {P1/P2/P3/P4}
Started: {TIME} UTC
Status: Investigating / Mitigating / Resolved

Affected: {Service/Endpoint}
Impact: {X% of requests failing | p99 latency +Y%}
Root Cause: {investigating}

Actions Taken:
- {action 1}
- {action 2}

ETA to Resolution: {if known}
```

---

## Incident Response Playbooks

### Playbook 1: High Error Rate

**Trigger**: Error rate > 1% for > 5 minutes

```
SEVERITY: P2 (usually) â†’ P1 (if > 5%)

IMMEDIATE (0-5 min):
  1. Check Grafana error dashboard
     â””â”€ Identify which endpoint is failing
     â””â”€ Check error type: 500 (server) vs 400 (client)
  
  2. Check logs
     kubectl logs -f deployment/nextgen-api -n production --tail=100
     
  3. Check database
     kubectl exec -it postgres-0 -c postgres -- \
       psql -U postgres -d nextgen_db -c "SELECT * FROM pg_stat_activity;"
     
     If connections > 80: Database is saturated

MITIGATION (5-15 min):
  Option A: Database saturation
    â””â”€ Scale API replicas down to 1
       kubectl scale deployment nextgen-api --replicas=1
    â””â”€ Wait 2 minutes, check if errors reduce
    â””â”€ If yes: Database connection pool issue (see Playbook 4)
    â””â”€ Scale back up gradually: replicas=3, wait 1min, then replicas=5

  Option B: Memory leak (memory increasing)
    â””â”€ Restart pods (causes brief downtime):
       kubectl rollout restart deployment/nextgen-api
    â””â”€ Monitor memory: kubectl top pods -n production
    â””â”€ If memory stable: Memory leak fixed by restart
    â””â”€ Investigate code review needed after incident

  Option C: Recent deployment
    â””â”€ Check last deployment:
       kubectl rollout history deployment/nextgen-api
    â””â”€ Rollback if committed < 30 min ago:
       kubectl rollout undo deployment/nextgen-api
    â””â”€ Wait 2 minutes, verify error rate drops
    â””â”€ Post-incident: Review deployment changes

VALIDATION (15+ min):
  âœ“ Error rate < baseline (0.1%)
  âœ“ No alerts firing
  âœ“ Users reporting no issues
  âœ“ All replicas healthy (kubectl get pods)

POST-INCIDENT:
  1. Create incident ticket in Jira
  2. Schedule postmortem (within 24 hours)
  3. Collect metrics: error_rate_max, duration, impact
```

### Playbook 2: High Latency

**Trigger**: p99 latency > 2 seconds for > 5 minutes

```
SEVERITY: P3 (if p99 > 2s), P2 (if p99 > 5s)

IMMEDIATE (0-5 min):
  1. Check which endpoint is slow
     kubectl exec -i prometheus-0 -c prometheus -- bash << 'EOF'
     curl -s 'http://localhost:9090/api/v1/query_range?query=\
       histogram_quantile(0.99,http_request_duration_seconds)&\
       start=1h&step=1m' | jq '.data.result[] | {metric, value}'
     EOF
  
  2. Check database query performance
     kubectl exec -it postgres-0 -c postgres -- \
       psql -U postgres -d nextgen_db << 'SQL'
     SELECT query, calls, mean_time, max_time FROM pg_stat_statements 
     ORDER BY mean_time DESC LIMIT 10;
     SQL
  
  3. Check resource usage
     kubectl top nodes
     kubectl top pods -n production

MITIGATION (5-15 min):
  Option A: Database slow queries
    â””â”€ Check slow query log:
       kubectl exec -it postgres-0 -c postgres -- \
         tail -100 /var/log/postgresql/postgresql.log | grep "duration"
    â””â”€ Scale database READ replicas if available
    â””â”€ Add database connection pooling (PgBouncer)
  
  Option B: API service slow (high CPU)
    â””â”€ Scale replicas up:
       kubectl scale deployment/nextgen-api --replicas=10
    â””â”€ Wait 1 minute, recheck p99 latency
    â””â”€ If improved: Load balancing issue, add more replicas
    â””â”€ If not improved: Not CPU-bound, investigate database/network
  
  Option C: Cache miss (high Redis latency)
    â””â”€ Check Redis connection pool:
       kubectl exec -it redis-0 -c redis -- \
         redis-cli INFO stats | grep -E "(total_commands|connected_clients)"
    â””â”€ Flush cache (cautious - will spike load):
       kubectl exec -it redis-0 -c redis -- redis-cli FLUSHDB
    â””â”€ Rebuild cache gradually (monitor error rate during this)

VALIDATION (15+ min):
  âœ“ p99 latency < 1s
  âœ“ p95 latency < 500ms
  âœ“ No new error spikes
  âœ“ CPU utilization < 70%

POST-INCIDENT:
  1. Analyze root cause in Prometheus
  2. Check if scaling needs adjustment
  3. Plan query optimization if DB was issue
```

### Playbook 3: Pod Restart Loop

**Trigger**: Pod restarting > 3 times in 5 minutes

```
SEVERITY: P2 (service degraded)

IMMEDIATE (0-5 min):
  1. Check pod status
     kubectl get pods -n production -w
  
  2. Check restart count
     kubectl get pods -n production -o custom-columns=\
       NAME:.metadata.name,RESTARTS:.status.containerStatuses[0].restartCount
  
  3. Check pod logs
     kubectl logs deployment/nextgen-api -n production --previous
     â””â”€ Note: --previous gets last terminated container logs

DIAGNOSIS (5-10 min):
  Check for common causes:
  
  A. OOMKilled (Out of Memory)
     â””â”€ kubectl describe pod {pod-name} -n production | grep "OOMKilled"
     â””â”€ Fix: Increase memory limit
        kubectl set resources deployment/nextgen-api \
          --limits=memory=2Gi -n production
  
  B. Liveness probe failing
     â””â”€ kubectl logs {pod-name} -n production | grep "health\|probe"
     â””â”€ Fix: Temporarily increase probe timeout
        kubectl patch deployment nextgen-api -n production -p \
          '{"spec":{"template":{"spec":{"containers":[{"name":"api","livenessProbe":{"initialDelaySeconds":60}}]}}}}'
  
  C. Crash on startup
     â””â”€ kubectl logs {pod-name} -n production | tail -50
     â””â”€ Look for: database connection error, config missing
     â””â”€ Fix: Verify ConfigMaps/Secrets exist
        kubectl get configmap,secret -n production

MITIGATION (10-20 min):
  1. Scale deployment to 0 (stop restart loop)
     kubectl scale deployment/nextgen-api --replicas=0
  
  2. Fix underlying issue (OOM, config, etc.)
  
  3. Scale back to desired replicas
     kubectl scale deployment/nextgen-api --replicas=3

VALIDATION:
  âœ“ Pod stable for > 5 minutes
  âœ“ No more restart events
  âœ“ Service responding to requests

POST-INCIDENT:
  1. Review pod events: kubectl describe pod {pod-name}
  2. Update resource limits if OOMKilled
  3. Review recent config changes
```

### Playbook 4: Database Connection Exhaustion

**Trigger**: DB connections > 90 of max 100

```
SEVERITY: P1 (queries start timing out)

IMMEDIATE (0-5 min):
  1. Check connection status
     kubectl exec -it postgres-0 -c postgres -- \
       psql -U postgres -d nextgen_db -c \
       "SELECT datname, usename, state, COUNT(*) FROM pg_stat_activity 
        GROUP BY datname, usename, state;"
  
  2. Identify long-running queries
     kubectl exec -it postgres-0 -c postgres -- \
       psql -U postgres -d nextgen_db -c \
       "SELECT pid, usename, query_start, query FROM pg_stat_activity 
        WHERE state != 'idle' ORDER BY query_start ASC;"

MITIGATION (5-15 min):
  Option A: Stale connections (idle for > 1 hour)
    â””â”€ Identify: state = 'idle' and query_start < NOW - 1 hour
    â””â”€ Terminate safely:
       SELECT pg_terminate_backend(pid) 
       FROM pg_stat_activity 
       WHERE state = 'idle' 
       AND query_start < NOW() - INTERVAL '1 hour'
       AND datname = 'nextgen_db';
  
  Option B: Long-running transaction
    â””â”€ Kill if > 30 min:
       SELECT pg_terminate_backend(pid) 
       FROM pg_stat_activity 
       WHERE (NOW() - query_start) > INTERVAL '30 minutes'
       AND state = 'active';
    â””â”€ Identify application issue that caused long transaction
  
  Option C: Connection pool saturation (normal)
    â””â”€ Add PgBouncer (connection pooler) if not present
    â””â”€ Increase max_connections in postgresql.conf:
       max_connections = 200  (from 100)
       shared_buffers = 512MB (increase for more connections)
    â””â”€ Apply with: ALTER SYSTEM SET max_connections = 200;
       SELECT pg_ctl('restart');

VALIDATION (15+ min):
  âœ“ Active connections < 20
  âœ“ Idle connections < 10
  âœ“ Query response time < 1s
  âœ“ No "FATAL: remaining connection slots reserved" errors

POST-INCIDENT:
  1. Review application connection handling
  2. Implement connection pooling (PgBouncer/PgPool)
  3. Set up alert for connection threshold
```

### Playbook 5: Disk Space Full

**Trigger**: Disk usage > 90%

```
SEVERITY: P1 (can cause crashes)

IMMEDIATE (0-5 min):
  1. Check disk usage
     kubectl exec -it master-node -- df -h
  
  2. Find large files
     kubectl exec -it master-node -- \
       du -sh /* | sort -rh | head -20

MITIGATION (5-20 min):
  Option A: Container logs filling disk
    â””â”€ Clear logs:
       kubectl delete pod -n production -l app=nextgen-api
    â””â”€ Note: New pods will start with clean logs
    â””â”€ Set log retention:
       kubectl patch ds filebeat -n kube-system -p \
       '{"spec":{"template":{"spec":{"containers":[{"name":"filebeat","env":[{"name":"LOGSTASH_FIELDS","value":"retention=7d"}]}]}}}}'
  
  Option B: etcd database growing
    â””â”€ Compact etcd:
       kubectl exec -it etcd-master -- \
         etcdctl compact $(etcdctl endpoint status --write-out=json | jq '.header.revision')
    â””â”€ Defragment:
       etcdctl defrag --endpoints=127.0.0.1:2379
  
  Option C: Persistent volume full
    â””â”€ Identify which PVC:
       kubectl get pvc -A
    â””â”€ Check what's consuming space:
       kubectl exec -it postgres-0 -- du -sh /var/lib/postgresql/data/*
    â””â”€ Expand PVC (if storage allows):
       kubectl patch pvc postgres-data -p '{"spec":{"resources":{"requests":{"storage":"100Gi"}}}}'
    â””â”€ Wait for storage expansion to complete
    â””â”€ Verify in pod: df -h /var/lib/postgresql/data

VALIDATION:
  âœ“ Disk usage < 70%
  âœ“ No more "No space left on device" errors
  âœ“ Services responding normally

POST-INCIDENT:
  1. Review disk usage trends
  2. Implement disk usage alerts at 75%
  3. Implement automatic log rotation (7-day retention)
  4. Plan storage expansion if growth continues
```

---

## Deployment Guide

### Pre-Deployment Checklist

```
â˜ Code Review
  â”œâ”€ Approved by 2+ reviewers
  â”œâ”€ No known security issues
  â””â”€ Passed all automated checks
  
â˜ Testing
  â”œâ”€ Unit tests: > 80% coverage
  â”œâ”€ Integration tests: Pass
  â”œâ”€ Staging environment: Tested
  â””â”€ Load test if critical change
  
â˜ Monitoring
  â”œâ”€ Metrics dashboard ready
  â”œâ”€ Alerts configured
  â”œâ”€ Log aggregation working
  â””â”€ Canary thresholds set
  
â˜ Rollback Plan
  â”œâ”€ Previous version identified
  â”œâ”€ Rollback procedure tested
  â””â”€ Team agrees on rollback criteria
  
â˜ Communication
  â”œâ”€ Team notified of deployment window
  â”œâ”€ On-call engineer assigned
  â””â”€ Stakeholders informed
```

### Deployment Steps

```
STEP 1: Merge to main branch
  â””â”€ git merge --ff-only feature/my-feature

STEP 2: Trigger GitHub Actions
  â””â”€ Workflow: ci-cd-enterprise.yml
  â””â”€ Stages: Lint â†’ Test â†’ Security â†’ Build â†’ Deploy Staging â†’ Deploy Prod

STEP 3: Monitor Staging (5 min)
  â””â”€ Check Grafana dashboard
  â””â”€ Verify no new errors
  â””â”€ Run smoke tests:
     curl -f https://api-staging.nextgen.local/health

STEP 4: Approve Production Deployment
  â””â”€ Requires code owner approval
  â””â”€ Check PR review comments
  â””â”€ Approve in GitHub Actions UI

STEP 5: Monitor Canary Deployment (10 min)
  â””â”€ 10% of traffic â†’ new version
  â””â”€ Metrics: error_rate < 0.5%, latency < 1.2x baseline
  â””â”€ If metrics fail â†’ Automatic rollback
  â””â”€ If metrics pass â†’ Proceed to 100%

STEP 6: Monitor Production (30 min)
  â””â”€ 100% traffic on new version
  â””â”€ Error rate normal
  â””â”€ Latency stable
  â””â”€ No customer complaints

STEP 7: Finaliz Deployment
  â””â”€ Mark as complete in GitHub Actions
  â””â”€ Post success message in #deployments
  â””â”€ Create incident ticket if any issues found
```

### Rollback Procedure

```
MANUAL ROLLBACK (if canary failed):
  1. Identify previous stable version
     kubectl rollout history deployment/nextgen-api
  
  2. Rollback immediately
     kubectl rollout undo deployment/nextgen-api
  
  3. Verify rollback
     kubectl rollout status deployment/nextgen-api
  
  4. Monitor for 5 minutes
     Grafana error rate, latency, pods
  
  5. Post incident notification
     "Rollback completed. Deployment reverted to previous version."

AUTOMATIC ROLLBACK (canary threshold exceeded):
  â””â”€ Triggered automatically by GitHub Actions
  â””â”€ Threshold: error_rate > 1% or latency > 2x baseline
  â””â”€ Duration: < 30 seconds
  â””â”€ Alert sent to on-call engineer
  â””â”€ Investigate before next deployment attempt
```

---

## Troubleshooting Steps

### General Troubleshooting Flow

```
1. Define the Problem
   â”œâ”€ What is broken? (API, database, cache, etc.)
   â”œâ”€ When did it start? (exact time)
   â”œâ”€ How many users affected? (% of traffic)
   â””â”€ What changed recently? (deployment, config, traffic)

2. Gather Evidence
   â”œâ”€ Logs: kubectl logs -f deployment/nextgen-api
   â”œâ”€ Metrics: Grafana dashboard
   â”œâ”€ Events: kubectl describe pod {pod-name}
   â””â”€ Configuration: kubectl get configmap,secret -A

3. Form Hypothesis
   â”œâ”€ Is it infrastructure? (nodes, storage, network)
   â”œâ”€ Is it application? (code, configuration, resources)
   â”œâ”€ Is it external? (database, cache, third-party)
   â””â”€ Is it user error? (misconfiguration, wrong endpoint)

4. Test Hypothesis
   â”œâ”€ Isolate the component
   â”œâ”€ Test connectivity
   â”œâ”€ Verify configuration
   â””â”€ Run health checks

5. Execute Fix
   â”œâ”€ Implement safest solution first
   â”œâ”€ Monitor impact
   â”œâ”€ Have rollback ready
   â””â”€ Document changes
```

### Common Issues & Fixes

```
ISSUE: "CrashLoopBackOff"
  Fix:
  1. Check logs: kubectl logs {pod} --previous
  2. Common causes:
     - Database not reachable â†’ Verify ConnectionString
     - Config missing â†’ kubectl get configmap -n production
     - OOMKilled â†’ Increase memory limits
  3. Restart cleanly:
     kubectl delete pod {pod} -n production

ISSUE: "Connection refused"
  Fix:
  1. Service not running: kubectl get svc -n production
  2. Pod not healthy: kubectl get pods -n production
  3. Port mismatch: kubectl get svc {service} -o yaml | grep port
  4. Network policy blocking: kubectl get networkpolicies -n production

ISSUE: "Timeout - Waiting for connection pool"
  Fix:
  1. Check database: kubectl get pod -n production -l app=postgres
  2. Check connections:
     SELECT COUNT(*) FROM pg_stat_activity;
  3. Kill stale connections (see Playbook 4)
  4. Increase pool size: max_connections = 200

ISSUE: "Disk space - mktemp: cannot create temp directory"
  Fix:
  1. Check disk: df -h /
  2. Find large files: du -sh /* | sort -rh | head -10
  3. Clear logs: kubectl logs -c log-aggregator {pod} > /dev/null
  4. Clean up: rm -rf /tmp/*

ISSUE: "Rate limit exceeded - Too many requests"
  Fix:
  1. Check rate limit config: kubectl get ingress -o yaml | grep limit
  2. Whitelist IP if needed: Add to limit-whitelist annotation
  3. Increase limit if legitimate: nginx.ingress.kubernetes.io/limit-rps: 200
  4. Check for DDoS: Look for single IP sending requests
```

---

## Escalation Paths

### Severity & Response Times

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ P1 - CRITICAL (Complete Outage)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Response: < 5 minutes                                   â”‚
â”‚ Escalation: On-call â†’ Team Lead â†’ Manager              â”‚
â”‚ Communication: Every 5 minutes                          â”‚
â”‚ Target Resolution: < 30 minutes                         â”‚
â”‚ Post-Incident: Postmortem within 24 hours             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ P2 - HIGH (Partial Degradation)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Response: < 15 minutes                                  â”‚
â”‚ Escalation: On-call â†’ Team Lead (if not resolved)     â”‚
â”‚ Communication: Every 15 minutes                         â”‚
â”‚ Target Resolution: < 2 hours                            â”‚
â”‚ Post-Incident: Postmortem within 48 hours             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ P3 - MEDIUM (Degraded Performance)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Response: < 30 minutes                                  â”‚
â”‚ Escalation: On-call â†’ Team Lead (if needed)            â”‚
â”‚ Communication: As needed                                â”‚
â”‚ Target Resolution: < 4 hours                            â”‚
â”‚ Post-Incident: Analysis during standup                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ P4 - LOW (Minor Issues)                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Response: < 1 day                                       â”‚
â”‚ Escalation: Track in Jira                              â”‚
â”‚ Communication: During standup                          â”‚
â”‚ Target Resolution: < 1 week                             â”‚
â”‚ Post-Incident: None required                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Escalation Contacts

```
Tier 1 (On-Call)
  â”œâ”€ @alice (Mon-Sun 09:00-17:00 UTC)
  â”œâ”€ On-call team in Slack: #on-call
  â””â”€ PagerDuty: nextgen-platform-oncall

Tier 2 (Team Lead)
  â”œâ”€ @bob (Backend Lead)
  â”œâ”€ @charlie (DevOps Lead)
  â””â”€ Slack: @team-leads

Tier 3 (Management)
  â”œâ”€ @diana (Engineering Manager)
  â”œâ”€ @eve (VP Engineering)
  â””â”€ Email: engineering-vp@nextgen.local
```

### When to Escalate

```
ESCALATE IMMEDIATELY if:
  âœ“ P1 severity (complete outage)
  âœ“ Cannot identify root cause within 10 minutes
  âœ“ Quick fix fails and alternative unknown
  âœ“ Requires infrastructure change (scale up, etc.)
  âœ“ Requires database migration or data change
  âœ“ Affects paying customers

ESCALATE WITHIN 15 MINUTES if:
  âœ“ P2 severity (partial outage)
  âœ“ On-call cannot fix within 30 minutes
  âœ“ Requires temporary workaround longer than 1 hour

ESCALATE WITHIN 1 HOUR if:
  âœ“ P3 severity (degraded performance)
  âœ“ Will require planned maintenance
  âœ“ Likely to recur without permanent fix
```

---

## On-Call Handoff Template

```
INCOMING ON-CALL ENGINEER:
  â˜ Check PagerDuty for active incidents
  â˜ Review Grafana dashboard trends (last 24 hours)
  â˜ Read Slack #incidents (last 7 days)
  â˜ Run health check:
     kubectl get nodes
     kubectl get pods -n production
  â˜ Verify communication channels working
  â˜ Confirm phone number in PagerDuty

OUTGOING ON-CALL ENGINEER:
  â˜ Brief incoming on any ongoing issues
  â˜ Transfer PagerDuty primary to incoming
  â˜ Review any recent incidents during shift
  â˜ Pass on any context not in documentation
  â˜ Confirm incoming ready to take over
```

---

## Status: âœ… Enterprise-Grade Operations Documentation

All runbooks, playbooks, and procedures documented for zero-error incident response.
